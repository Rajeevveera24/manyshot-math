{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the 'src' directory to sys.path\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
    "\n",
    "# Import math_eval from the utils package\n",
    "from utils import math_eval\n",
    "\n",
    "# os.environ ['CUDA_LAUNCH_BLOCKING'] ='1'\n",
    "# os.environ['VLLM_LOGGING_LEVEL'] = 'DEBUG'\n",
    "# os.environ['NCCL_P2P_DISABLE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_math(path=\"../datasets/MATH\", split=\"train\"):\n",
    "    with open(os.path.join(path, split, \"dataset.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    examples = [{\n",
    "        'question': q,\n",
    "        'answer': a,\n",
    "    } for q, a in zip(data['question'], data['extracted_answers'])]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_math(split='test')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 16:04:48 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 11-29 16:04:48 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-29 16:04:48 config.py:1020] Defaulting to use mp for distributed inference\n",
      "WARNING 11-29 16:04:48 arg_utils.py:1013] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 11-29 16:04:48 config.py:1136] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 11-29 16:04:48 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=50000, download_dir='/home/amittur/.cache/huggingface/hub', load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 11-29 16:04:48 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-29 16:04:49 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 11-29 16:04:49 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:49 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-29 16:04:50 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 11-29 16:04:50 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:50 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:50 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 11-29 16:04:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/amittur/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:51 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/amittur/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 11-29 16:04:51 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f5d8a8143d0>, local_subscribe_port=47905, remote_subscribe_port=None)\n",
      "INFO 11-29 16:04:51 model_runner.py:1072] Starting to load model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:51 model_runner.py:1072] Starting to load model hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4...\n",
      "INFO 11-29 16:04:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:04:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8943b29b54042328bd8333db4ccbb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-29 16:06:33 model_runner.py:1077] Loading model weights took 18.5799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:06:33 model_runner.py:1077] Loading model weights took 18.5799 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:06:34 worker.py:232] Memory profiling results: total_gpu_memory=44.42GiB initial_memory_usage=19.32GiB peak_torch_memory=18.70GiB memory_usage_post_profile=19.44GiB non_torch_memory=0.85GiB kv_cache_size=20.43GiB gpu_memory_utilization=0.90\n",
      "INFO 11-29 16:06:34 worker.py:232] Memory profiling results: total_gpu_memory=44.42GiB initial_memory_usage=19.32GiB peak_torch_memory=19.77GiB memory_usage_post_profile=19.46GiB non_torch_memory=0.86GiB kv_cache_size=19.35GiB gpu_memory_utilization=0.90\n",
      "INFO 11-29 16:06:34 distributed_gpu_executor.py:57] # GPU blocks: 7925, # CPU blocks: 1638\n",
      "INFO 11-29 16:06:34 distributed_gpu_executor.py:61] Maximum concurrency for 50000 tokens per request: 2.54x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:06:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:06:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-29 16:06:41 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-29 16:06:41 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-29 16:07:00 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:07:01 custom_all_reduce.py:224] Registering 5635 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3837248)\u001b[0;0m INFO 11-29 16:07:01 model_runner.py:1518] Graph capturing finished in 21 secs, took 0.50 GiB\n",
      "INFO 11-29 16:07:01 model_runner.py:1518] Graph capturing finished in 21 secs, took 0.50 GiB\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    # model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    model=\"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\", \n",
    "    tensor_parallel_size=2,\n",
    "    download_dir=\"/home/amittur/.cache/huggingface/hub\", \n",
    "    max_model_len=50000,\n",
    "    # gpu_memory_utilization=0.99,\n",
    "    # max_num_seqs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solve_prompts(data):\n",
    "    SOLVE_PROMPT = \"\"\"Answer the math problem in the format shown below. End your response with \"<|eot_id|>\".\n",
    "\n",
    "---\n",
    "Problem: <you will be given a math problem> \n",
    "Reasoning: <your step by step reasoning for the answer>\n",
    "Answer: <your final answer only>\n",
    "---\n",
    "\n",
    "Problem: {}\n",
    "\"\"\"\n",
    "\n",
    "    return [SOLVE_PROMPT.format(d['question']) for d in data]\n",
    "\n",
    "\n",
    "def get_n_shot_cot_solve_prompts(data, synth_data):\n",
    "    SOLVE_N_SHOT_PROMPT = \"\"\"Some examples of math problems and their answers, similar to the one you are going to be asked are provided below. Use them to understand and solve the problem. End your response with \"<|eot_id|>\".    \n",
    "\n",
    "### Examples ###\n",
    "{examples}\n",
    "\n",
    "### Output Format ###\n",
    "Problem: <the math problem you need to solve> \n",
    "Reasoning: <your step by step reasoning for the answer>\n",
    "Answer: <your final answer only>\n",
    "\n",
    "### Input ###\n",
    "Problem: {problem}\n",
    "\"\"\"\n",
    "    return [\n",
    "        SOLVE_N_SHOT_PROMPT.format(\n",
    "            examples=\"\\n\\n\".join([f\"Problem: {d['problem']}\\nAnswer: {d['answer']}\" for d in synth_data[i]]), \n",
    "            # examples=\"\\n\\n\".join([f\"Problem: {d['problem']}\" for d in synth_data[i]]), \n",
    "            problem=d['question']\n",
    "        ) \n",
    "        for i, d in enumerate(data)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_generate_prompt(data, n_samples=1):\n",
    "    GENERATE_PROMPT = \"\"\"Given a reference math problem, generate {n_samples} similar math problems, along with their answers. End your response with \"<|eot_id|>\".\n",
    "Use this format for each generated problem:\n",
    "Problem: <new math problem>\n",
    "Answer: <answer to the math problem>\n",
    "\n",
    "Problem: {problem}\n",
    "\"\"\"\n",
    "\n",
    "    return [GENERATE_PROMPT.format(n_samples=n_samples, problem=d['question']) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=5120,\n",
    "    stop_token_ids=[128001, 128008, 128009],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cot_answer(text):\n",
    "    match = re.search(r\"Reasoning: (.+)Answer: (.+)\", text, re.DOTALL)\n",
    "    answer = reasoning = ''\n",
    "    if match:\n",
    "        reasoning = match.group(1).strip('\\n ')\n",
    "        answer = match.group(2).strip('\\n ')\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_synthetic_data(text):\n",
    "    matches = re.findall(r\"Problem: (.*?)\\nAnswer: (.*?)(?=\\s*Problem:|$)\", text, re.DOTALL)\n",
    "    return [{\n",
    "        'problem': q.strip('\\n '),\n",
    "        'answer': a.strip('\\n ')\n",
    "    } for q, a in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:12<00:00, 12.93s/it, est. speed input: 10.52 toks/s, output: 31.08 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a reference math problem, generate 4 similar math problems, along with their answers. End your response with \"<|eot_id|>\".\n",
      "Use this format for each generated problem:\n",
      "Problem: <new math problem>\n",
      "Answer: <answer to the math problem>\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{10}$, 2 with a probability of $\\frac{2}{10}$, 3 with a probability of $\\frac{3}{10}$, and 4 with a probability of $\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      " Answer: $\\frac{1}{25}$\n",
      "\n",
      "Here are 4 similar math problems:\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{8}$, 2 with a probability of $\\frac{2}{8}$, 3 with a probability of $\\frac{3}{8}$, and 4 with a probability of $\\frac{2}{8}$. If Alex and Ben both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{16}$\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{6}$, 2 with a probability of $\\frac{2}{6}$, 3 with a probability of $\\frac{2}{6}$, and 4 with a probability of $\\frac{1}{6}$. If Charlie and David both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{9}$\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{12}$, 2 with a probability of $\\frac{3}{12}$, 3 with a probability of $\\frac{4}{12}$, and 4 with a probability of $\\frac{4}{12}$. If Emily and Frank both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{36}$\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{15}$, 2 with a probability of $\\frac{4}{15}$, 3 with a probability of $\\frac{6}{15}$, and 4 with a probability of $\\frac{4}{15}$. If George and Hannah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{225}$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'problem': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{8}$, 2 with a probability of $\\\\frac{2}{8}$, 3 with a probability of $\\\\frac{3}{8}$, and 4 with a probability of $\\\\frac{2}{8}$. If Alex and Ben both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{16}$'},\n",
       " {'problem': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{6}$, 2 with a probability of $\\\\frac{2}{6}$, 3 with a probability of $\\\\frac{2}{6}$, and 4 with a probability of $\\\\frac{1}{6}$. If Charlie and David both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{9}$'},\n",
       " {'problem': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{12}$, 2 with a probability of $\\\\frac{3}{12}$, 3 with a probability of $\\\\frac{4}{12}$, and 4 with a probability of $\\\\frac{4}{12}$. If Emily and Frank both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{36}$'},\n",
       " {'problem': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{15}$, 2 with a probability of $\\\\frac{4}{15}$, 3 with a probability of $\\\\frac{6}{15}$, and 4 with a probability of $\\\\frac{4}{15}$. If George and Hannah both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{225}$'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "q_idx = 6\n",
    "\n",
    "outputs = llm.generate(get_generate_prompt([data[q_idx]], 4), sampling_params)\n",
    "\n",
    "print(outputs[0].prompt, outputs[0].outputs[0].text)\n",
    "synth_data = extract_synthetic_data(outputs[0].outputs[0].text)\n",
    "synth_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of math problems and their answers, similar to the one you are going to be asked are provided below. Use them to understand and solve the problem. End your response with \"<|eot_id|>\".    \n",
      "\n",
      "### Examples ###\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{8}$, 2 with a probability of $\\frac{2}{8}$, 3 with a probability of $\\frac{3}{8}$, and 4 with a probability of $\\frac{2}{8}$. If Alex and Ben both spin the spinner, what is the probability that they get the same number?\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{6}$, 2 with a probability of $\\frac{2}{6}$, 3 with a probability of $\\frac{2}{6}$, and 4 with a probability of $\\frac{1}{6}$. If Charlie and David both spin the spinner, what is the probability that they get the same number?\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{12}$, 2 with a probability of $\\frac{3}{12}$, 3 with a probability of $\\frac{4}{12}$, and 4 with a probability of $\\frac{4}{12}$. If Emily and Frank both spin the spinner, what is the probability that they get the same number?\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{15}$, 2 with a probability of $\\frac{4}{15}$, 3 with a probability of $\\frac{6}{15}$, and 4 with a probability of $\\frac{4}{15}$. If George and Hannah both spin the spinner, what is the probability that they get the same number?\n",
      "\n",
      "### Output Format ###\n",
      "Problem: <the math problem you need to solve> \n",
      "Reasoning: <your step by step reasoning for the answer>\n",
      "Answer: <your final answer only>\n",
      "\n",
      "### Input ###\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{10}$, 2 with a probability of $\\frac{2}{10}$, 3 with a probability of $\\frac{3}{10}$, and 4 with a probability of $\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_n_shot_cot_solve_prompts([data[q_idx]], [synth_data])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:03<00:00,  3.23s/it, est. speed input: 141.69 toks/s, output: 70.54 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of math problems and their answers, similar to the one you are going to be asked are provided below. Use them to understand and solve the problem. End your response with \"<|eot_id|>\".    \n",
      "\n",
      "### Examples ###\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{8}$, 2 with a probability of $\\frac{2}{8}$, 3 with a probability of $\\frac{3}{8}$, and 4 with a probability of $\\frac{4}{8}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{4}$\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{12}$, 2 with a probability of $\\frac{2}{12}$, 3 with a probability of $\\frac{3}{12}$, and 4 with a probability of $\\frac{4}{12}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{6}$\n",
      "\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{16}$, 2 with a probability of $\\frac{2}{16}$, 3 with a probability of $\\frac{3}{16}$, and 4 with a probability of $\\frac{4}{16}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{8}$\n",
      "\n",
      "### Output Format ###\n",
      "Problem: <the math problem you need to solve> \n",
      "Reasoning: <your step by step reasoning for the answer>\n",
      "Answer: <your final answer only>\n",
      "\n",
      "### Input ###\n",
      "Problem: I have a spinner that lands on 1 with a probability of $\\frac{1}{10}$, 2 with a probability of $\\frac{2}{10}$, 3 with a probability of $\\frac{3}{10}$, and 4 with a probability of $\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      " Reasoning: To find the probability that Phil and Sarah get the same number, we need to find the probability of each possible outcome and add them together. The probability of both getting 1 is $\\frac{1}{10} \\cdot \\frac{1}{10} = \\frac{1}{100}$. The probability of both getting 2 is $\\frac{2}{10} \\cdot \\frac{2}{10} = \\frac{4}{100}$. The probability of both getting 3 is $\\frac{3}{10} \\cdot \\frac{3}{10} = \\frac{9}{100}$. The probability of both getting 4 is $\\frac{4}{10} \\cdot \\frac{4}{10} = \\frac{16}{100}$. Adding these probabilities together, we get $\\frac{1}{100} + \\frac{4}{100} + \\frac{9}{100} + \\frac{16}{100} = \\frac{30}{100} = \\frac{3}{10}$.\n",
      "Answer: $\\frac{3}{10}$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': '$\\\\frac{3}{10}$',\n",
       " 'reasoning': 'To find the probability that Phil and Sarah get the same number, we need to find the probability of each possible outcome and add them together. The probability of both getting 1 is $\\\\frac{1}{10} \\\\cdot \\\\frac{1}{10} = \\\\frac{1}{100}$. The probability of both getting 2 is $\\\\frac{2}{10} \\\\cdot \\\\frac{2}{10} = \\\\frac{4}{100}$. The probability of both getting 3 is $\\\\frac{3}{10} \\\\cdot \\\\frac{3}{10} = \\\\frac{9}{100}$. The probability of both getting 4 is $\\\\frac{4}{10} \\\\cdot \\\\frac{4}{10} = \\\\frac{16}{100}$. Adding these probabilities together, we get $\\\\frac{1}{100} + \\\\frac{4}{100} + \\\\frac{9}{100} + \\\\frac{16}{100} = \\\\frac{30}{100} = \\\\frac{3}{10}$.'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(get_n_shot_cot_solve_prompts([data[q_idx]], [synth_data]), sampling_params)\n",
    "\n",
    "print(outputs[0].prompt, outputs[0].outputs[0].text)\n",
    "result = extract_cot_answer(outputs[0].outputs[0].text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{10}$, 2 with a probability of $\\\\frac{2}{10}$, 3 with a probability of $\\\\frac{3}{10}$, and 4 with a probability of $\\\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?',\n",
       " 'answer': '\\\\dfrac{3}{10}'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[q_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 1}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check eval\n",
    "\n",
    "math_eval.process_results(data[q_idx], result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_prompts = get_generate_prompt(data[:5], 3)\n",
    "generate_prompts = get_generate_prompt(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/_synth_data_70b_int4_5.json', 'r') as f:\n",
    "    synth_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve_n_shot_prompts = get_n_shot_solve_prompts(data[:5], synth_data)\n",
    "solve_n_shot_prompts = get_n_shot_cot_solve_prompts(data, synth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|█▉        | 998/5000 [05:47<43:49,  1.52it/s, est. speed input: 1383.35 toks/s, output: 652.94 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 21:51:56 scheduler.py:1481] Sequence group 1254 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|██        | 1007/5000 [06:00<2:49:17,  2.54s/it, est. speed input: 1347.30 toks/s, output: 638.81 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 21:52:29 scheduler.py:1481] Sequence group 1122 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n",
      "WARNING 11-27 21:53:50 scheduler.py:1481] Sequence group 916 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n",
      "WARNING 11-27 21:56:38 scheduler.py:1481] Sequence group 718 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  28%|██▊       | 1404/5000 [15:12<4:41:17,  4.69s/it, est. speed input: 730.78 toks/s, output: 999.54 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 22:01:27 scheduler.py:1481] Sequence group 1446 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▌    | 2792/5000 [25:59<47:59,  1.30s/it, est. speed input: 849.62 toks/s, output: 1221.65 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 22:12:10 scheduler.py:1481] Sequence group 2965 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  56%|█████▌    | 2803/5000 [27:21<3:06:44,  5.10s/it, est. speed input: 810.27 toks/s, output: 1190.32 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 22:13:36 scheduler.py:1481] Sequence group 2411 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  90%|█████████ | 4510/5000 [38:08<13:26,  1.65s/it, est. speed input: 928.31 toks/s, output: 1273.13 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 22:24:18 scheduler.py:1481] Sequence group 4694 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████ | 4530/5000 [39:53<1:10:53,  9.05s/it, est. speed input: 892.30 toks/s, output: 1247.74 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-27 22:26:05 scheduler.py:1481] Sequence group 4236 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5000/5000 [47:22<00:00,  1.76it/s, est. speed input: 826.85 toks/s, output: 1378.95 toks/s]  \n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "# for i in tqdm(range(0, len(solve_n_shot_prompts), BATCH_SIZE)):\n",
    "# outputs = llm.generate(solve_n_shot_prompts[i:i+BATCH_SIZE], sampling_params)\n",
    "outputs = llm.generate(solve_n_shot_prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    answers.append(extract_cot_answer(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save answers as json\n",
    "with open(\"answers_5_shot_synth_8b_v2.json\", \"w\") as f:\n",
    "    json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manyshot-math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
