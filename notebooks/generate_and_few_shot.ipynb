{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve().parent / \"src\"))\n",
    "\n",
    "from utils import math_eval\n",
    "\n",
    "os.environ ['CUDA_LAUNCH_BLOCKING'] ='1'\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'DEBUG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_math(path=\"../datasets/MATH\", split=\"train\"):\n",
    "    with open(os.path.join(path, split, \"dataset.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    examples = [{\n",
    "        'question': q,\n",
    "        'answer': a,\n",
    "    } for q, a in zip(data['question'], data['extracted_answers'])]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_math(split='test')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-25 09:17:46 config.py:350] This model supports multiple tasks: {'generate', 'embedding'}. Defaulting to 'generate'.\n",
      "INFO 11-25 09:17:46 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=20000, download_dir='/home/amittur/.cache/huggingface/hub', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "INFO 11-25 09:17:47 selector.py:135] Using Flash Attention backend.\n",
      "INFO 11-25 09:17:47 model_runner.py:1072] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 11-25 09:17:48 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a8db8765c54ab2b3ff7ceb00a617af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-25 09:17:51 model_runner.py:1077] Loading model weights took 14.9888 GB\n",
      "INFO 11-25 09:17:53 worker.py:232] Memory profiling results: total_gpu_memory=39.50GiB initial_memory_usage=15.49GiB peak_torch_memory=17.06GiB memory_usage_post_profile=15.51GiB non_torch_memory=0.51GiB kv_cache_size=21.53GiB gpu_memory_utilization=0.99\n",
      "INFO 11-25 09:17:53 gpu_executor.py:113] # GPU blocks: 11023, # CPU blocks: 2048\n",
      "INFO 11-25 09:17:53 gpu_executor.py:117] Maximum concurrency for 20000 tokens per request: 8.82x\n",
      "INFO 11-25 09:17:56 model_runner.py:1400] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-25 09:17:56 model_runner.py:1404] If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 11-25 09:17:58 model_runner.py:1518] Graph capturing finished in 2 secs, took 0.07 GiB\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    download_dir=\"/home/amittur/.cache/huggingface/hub\", \n",
    "    max_model_len=20000,\n",
    "    gpu_memory_utilization=0.99,\n",
    "    max_num_seqs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solve_prompts(data):\n",
    "    SOLVE_PROMPT = \"\"\"Answer the math problem in the format shown below. End your response with \"<|eot_id|>\".\n",
    "\n",
    "---\n",
    "Question: <you will be given a math question> \n",
    "Reasoning: <your step by step reasoning for the answer>\n",
    "Answer: <your final answer only>\n",
    "---\n",
    "\n",
    "Question: {}\n",
    "\"\"\"\n",
    "\n",
    "    return [SOLVE_PROMPT.format(d['question']) for d in data]\n",
    "\n",
    "\n",
    "def get_n_shot_cot_solve_prompts(data, synth_data):\n",
    "    SOLVE_N_SHOT_PROMPT = \"\"\"Some examples of math problems and their answers, similar to the one you are going to be asked are provided below. Use them to understand and solve the problem. End your response with \"<|eot_id|>\".    \n",
    "\n",
    "### Examples ###\n",
    "{examples}\n",
    "\n",
    "### Output Format ###\n",
    "Question: <the math problem you need to solve> \n",
    "Reasoning: <your step by step reasoning for the answer>\n",
    "Answer: <your final answer only>\n",
    "\n",
    "### Input ###\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "    return [\n",
    "        SOLVE_N_SHOT_PROMPT.format(\n",
    "            examples=\"\\n\\n\".join([f\"Question: {d['question']}\\nAnswer: {d['answer']}\" for d in synth_data[i]]), \n",
    "            question=d['question']\n",
    "        ) \n",
    "        for i, d in enumerate(data)\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_generate_prompt(data, n_samples=1):\n",
    "    GENERATE_PROMPT = \"\"\"Given a reference math problem, generate {n_samples} similar math problems, along with their answers. End your response with \"<|eot_id|>\".\n",
    "Use this format for each generated problem:\n",
    "Question: <new math problem>\n",
    "Answer: <answer to the math problem>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "    return [GENERATE_PROMPT.format(n_samples=n_samples, question=d['question']) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=5120,\n",
    "    stop_token_ids=[128001, 128008, 128009],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cot_answer(text):\n",
    "    match = re.search(r\"Reasoning: (.+)Answer: (.+)\", text, re.DOTALL)\n",
    "    answer = reasoning = ''\n",
    "    if match:\n",
    "        reasoning = match.group(1).strip('\\n ')\n",
    "        answer = match.group(2).strip('\\n ')\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "\n",
    "def extract_synthetic_data(text):\n",
    "    matches = re.findall(r\"Question: (.*?)\\nAnswer: (.*?)(?=\\s*Question:|$)\", text, re.DOTALL)\n",
    "    return [{\n",
    "        'question': q.strip('\\n '),\n",
    "        'answer': a.strip('\\n ')\n",
    "    } for q, a in matches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it, est. speed input: 31.64 toks/s, output: 69.32 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a reference math problem, generate 4 similar math problems, along with their answers. End your response with \"<|eot_id|>\".\n",
      "Use this format for each generated problem:\n",
      "Question: <new math problem>\n",
      "Answer: <answer to the math problem>\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{10}$, 2 with a probability of $\\frac{2}{10}$, 3 with a probability of $\\frac{3}{10}$, and 4 with a probability of $\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      " Answer: $\\frac{1}{5}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{8}$, 2 with a probability of $\\frac{2}{8}$, 3 with a probability of $\\frac{3}{8}$, and 4 with a probability of $\\frac{4}{8}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{4}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{12}$, 2 with a probability of $\\frac{2}{12}$, 3 with a probability of $\\frac{3}{12}$, and 4 with a probability of $\\frac{4}{12}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{6}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{16}$, 2 with a probability of $\\frac{2}{16}$, 3 with a probability of $\\frac{3}{16}$, and 4 with a probability of $\\frac{4}{16}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{8}$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{8}$, 2 with a probability of $\\\\frac{2}{8}$, 3 with a probability of $\\\\frac{3}{8}$, and 4 with a probability of $\\\\frac{4}{8}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{4}$'},\n",
       " {'question': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{12}$, 2 with a probability of $\\\\frac{2}{12}$, 3 with a probability of $\\\\frac{3}{12}$, and 4 with a probability of $\\\\frac{4}{12}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{6}$'},\n",
       " {'question': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{16}$, 2 with a probability of $\\\\frac{2}{16}$, 3 with a probability of $\\\\frac{3}{16}$, and 4 with a probability of $\\\\frac{4}{16}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?',\n",
       "  'answer': '$\\\\frac{1}{8}$'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check\n",
    "q_idx = 6\n",
    "\n",
    "outputs = llm.generate(get_generate_prompt([data[q_idx]], 4), sampling_params)\n",
    "\n",
    "print(outputs[0].prompt, outputs[0].outputs[0].text)\n",
    "synth_data = extract_synthetic_data(outputs[0].outputs[0].text)\n",
    "synth_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of math problems and their answers, similar to the one you are going to be asked are provided below. Use them to understand and solve the problem. End your response with \"<|eot_id|>\".    \n",
      "\n",
      "### Examples ###\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{8}$, 2 with a probability of $\\frac{2}{8}$, 3 with a probability of $\\frac{3}{8}$, and 4 with a probability of $\\frac{4}{8}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{4}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{12}$, 2 with a probability of $\\frac{2}{12}$, 3 with a probability of $\\frac{3}{12}$, and 4 with a probability of $\\frac{4}{12}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{6}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{16}$, 2 with a probability of $\\frac{2}{16}$, 3 with a probability of $\\frac{3}{16}$, and 4 with a probability of $\\frac{4}{16}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{8}$\n",
      "\n",
      "### Output Format ###\n",
      "Question: <the math problem you need to solve> \n",
      "Reasoning: <your step by step reasoning for the answer>\n",
      "Answer: <your final answer only>\n",
      "\n",
      "### Input ###\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{10}$, 2 with a probability of $\\frac{2}{10}$, 3 with a probability of $\\frac{3}{10}$, and 4 with a probability of $\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(get_n_shot_cot_solve_prompts([data[q_idx]], [synth_data])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.09s/it, est. speed input: 111.93 toks/s, output: 69.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some examples of math problems and their answers, similar to the one you are going to be asked are provided below. Use them to understand and solve the problem. End your response with \"<|eot_id|>\".    \n",
      "\n",
      "### Examples ###\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{8}$, 2 with a probability of $\\frac{2}{8}$, 3 with a probability of $\\frac{3}{8}$, and 4 with a probability of $\\frac{4}{8}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{4}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{12}$, 2 with a probability of $\\frac{2}{12}$, 3 with a probability of $\\frac{3}{12}$, and 4 with a probability of $\\frac{4}{12}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{6}$\n",
      "\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{16}$, 2 with a probability of $\\frac{2}{16}$, 3 with a probability of $\\frac{3}{16}$, and 4 with a probability of $\\frac{4}{16}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      "Answer: $\\frac{1}{8}$\n",
      "\n",
      "### Output Format ###\n",
      "Question: <the math problem you need to solve> \n",
      "Reasoning: <your step by step reasoning for the answer>\n",
      "Answer: <your final answer only>\n",
      "\n",
      "### Input ###\n",
      "Question: I have a spinner that lands on 1 with a probability of $\\frac{1}{10}$, 2 with a probability of $\\frac{2}{10}$, 3 with a probability of $\\frac{3}{10}$, and 4 with a probability of $\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?\n",
      " Reasoning: To find the probability that Phil and Sarah get the same number, we need to find the probability of each possible outcome where they get the same number and add them up. The possible outcomes are: both land on 1, both land on 2, both land on 3, and both land on 4. The probability of each outcome is the product of the individual probabilities of each event. So, the probability of both landing on 1 is $\\frac{1}{10} * \\frac{1}{10} = \\frac{1}{100}$, the probability of both landing on 2 is $\\frac{2}{10} * \\frac{2}{10} = \\frac{4}{100}$, the probability of both landing on 3 is $\\frac{3}{10} * \\frac{3}{10} = \\frac{9}{100}$, and the probability of both landing on 4 is $\\frac{4}{10} * \\frac{4}{10} = \\frac{16}{100}$. Adding these probabilities together, we get $\\frac{1}{100} + \\frac{4}{100} + \\frac{9}{100} + \\frac{16}{100} = \\frac{30}{100} = \\frac{3}{10}$.\n",
      "Answer: $\\frac{3}{10}$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'answer': '$\\\\frac{3}{10}$',\n",
       " 'reasoning': 'To find the probability that Phil and Sarah get the same number, we need to find the probability of each possible outcome where they get the same number and add them up. The possible outcomes are: both land on 1, both land on 2, both land on 3, and both land on 4. The probability of each outcome is the product of the individual probabilities of each event. So, the probability of both landing on 1 is $\\\\frac{1}{10} * \\\\frac{1}{10} = \\\\frac{1}{100}$, the probability of both landing on 2 is $\\\\frac{2}{10} * \\\\frac{2}{10} = \\\\frac{4}{100}$, the probability of both landing on 3 is $\\\\frac{3}{10} * \\\\frac{3}{10} = \\\\frac{9}{100}$, and the probability of both landing on 4 is $\\\\frac{4}{10} * \\\\frac{4}{10} = \\\\frac{16}{100}$. Adding these probabilities together, we get $\\\\frac{1}{100} + \\\\frac{4}{100} + \\\\frac{9}{100} + \\\\frac{16}{100} = \\\\frac{30}{100} = \\\\frac{3}{10}$.'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = llm.generate(get_n_shot_cot_solve_prompts([data[q_idx]], [synth_data]), sampling_params)\n",
    "\n",
    "print(outputs[0].prompt, outputs[0].outputs[0].text)\n",
    "result = extract_cot_answer(outputs[0].outputs[0].text)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'I have a spinner that lands on 1 with a probability of $\\\\frac{1}{10}$, 2 with a probability of $\\\\frac{2}{10}$, 3 with a probability of $\\\\frac{3}{10}$, and 4 with a probability of $\\\\frac{4}{10}$. If Phil and Sarah both spin the spinner, what is the probability that they get the same number?',\n",
       " 'answer': '\\\\dfrac{3}{10}'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[q_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check eval\n",
    "\n",
    "math_eval.process_results(data[q_idx], result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_prompts = get_generate_prompt(data[:5], 3)\n",
    "generate_prompts = get_generate_prompt(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "# synth_data = []\n",
    "\n",
    "# for i in tqdm(range(0, len(generate_prompts), BATCH_SIZE)):\n",
    "#     outputs = llm.generate(generate_prompts[i:i+BATCH_SIZE], sampling_params)\n",
    "#     for output in outputs:\n",
    "#         generated_text = output.outputs[0].text\n",
    "#         synth_data.append(extract_synthetic_data(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/synth_data.json', 'r') as f:\n",
    "    synth_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve_n_shot_prompts = get_n_shot_solve_prompts(data[:5], synth_data)\n",
    "solve_n_shot_prompts = get_n_shot_cot_solve_prompts(data, synth_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   2%|▏         | 99/5000 [00:37<38:12,  2.14it/s, est. speed input: 864.36 toks/s, output: 435.53 toks/s]  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m answers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# for i in tqdm(range(0, len(solve_n_shot_prompts), BATCH_SIZE)):\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# outputs = llm.generate(solve_n_shot_prompts[i:i+BATCH_SIZE], sampling_params)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolve_n_shot_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[1;32m      7\u001b[0m     generated_text \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/utils.py:1063\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1056\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1059\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1060\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m         )\n\u001b[0;32m-> 1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/entrypoints/llm.py:406\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    396\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m SamplingParams()\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    399\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    400\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    404\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 406\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/entrypoints/llm.py:942\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m    940\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m--> 942\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m    944\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1454\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1451\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1452\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1454\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:125\u001b[0m, in \u001b[0;36mGPUExecutor.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m    124\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 125\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/worker/worker_base.py:343\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    339\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    340\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    341\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 343\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/worker/model_runner_base.py:116\u001b[0m, in \u001b[0;36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    118\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/worker/model_runner.py:1697\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     model_input\u001b[38;5;241m.\u001b[39masync_callback()\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[0;32m-> 1697\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1702\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_forward_time\n\u001b[1;32m   1703\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1704\u001b[0m     model_forward_end\u001b[38;5;241m.\u001b[39msynchronize()\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:577\u001b[0m, in \u001b[0;36mLlamaForCausalLM.sample\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    576\u001b[0m            sampling_metadata: SamplingMetadata) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[SamplerOutput]:\n\u001b[0;32m--> 577\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/manyshot-math/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:270\u001b[0m, in \u001b[0;36mSampler.forward\u001b[0;34m(self, logits, sampling_metadata)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;66;03m# Use float32 to apply temperature scaling.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Use in-place division to avoid creating a new tensor.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m--> 270\u001b[0m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampling_tensors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemperatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_top_p_top_k \u001b[38;5;129;01mand\u001b[39;00m flashinfer_top_k_top_p_sampling \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m     logits \u001b[38;5;241m=\u001b[39m _apply_top_k_top_p(logits, sampling_tensors\u001b[38;5;241m.\u001b[39mtop_ps,\n\u001b[1;32m    274\u001b[0m                                 sampling_tensors\u001b[38;5;241m.\u001b[39mtop_ks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "# for i in tqdm(range(0, len(solve_n_shot_prompts), BATCH_SIZE)):\n",
    "# outputs = llm.generate(solve_n_shot_prompts[i:i+BATCH_SIZE], sampling_params)\n",
    "outputs = llm.generate(solve_n_shot_prompts, sampling_params)\n",
    "for output in outputs:\n",
    "    generated_text = output.outputs[0].text\n",
    "    answers.append(extract_cot_answer(generated_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save answers as json\n",
    "with open(\"answers_3_shot_synth.json\", \"w\") as f:\n",
    "    json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manyshot-math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
