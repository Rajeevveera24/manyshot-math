{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2\"\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# os.environ[\"VLLM_LOGGING_LEVEL\"] = \"DEBUG\"\n",
    "# os.environ[\"NCCL_P2P_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_math(path=\"../datasets/MATH\", split=\"train\"):\n",
    "    with open(os.path.join(path, split, \"dataset.json\")) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    examples = [{\n",
    "        'question': q,\n",
    "        'answer': a,\n",
    "    } for q, a in zip(data['question'], data['extracted_answers'])]\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_math(split='test')\n",
    "# data = load_math(split='train') + load_math(split='test')\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-27 11:56:14 config.py:350] This model supports multiple tasks: {'embedding', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 11-27 11:56:14 awq_marlin.py:109] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-27 11:56:15 config.py:1020] Defaulting to use mp for distributed inference\n",
      "INFO 11-27 11:56:15 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', speculative_config=None, tokenizer='hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=10000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n",
      "WARNING 11-27 11:56:15 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-27 11:56:15 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 11-27 11:56:15 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3580231)\u001b[0;0m INFO 11-27 11:56:15 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-27 11:56:15 selector.py:135] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3580231)\u001b[0;0m INFO 11-27 11:56:22 utils.py:961] Found nccl from library libnccl.so.2\n",
      "INFO 11-27 11:56:22 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3580231)\u001b[0;0m INFO 11-27 11:56:22 utils.py:961] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3580231)\u001b[0;0m INFO 11-27 11:56:22 pynccl.py:69] vLLM is using nccl==2.21.5\n"
     ]
    }
   ],
   "source": [
    "llm = LLM(\n",
    "    # model=\"meta-llama/Llama-3.1-8B-Instruct\", \n",
    "    model=\"hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\", \n",
    "    tensor_parallel_size=2,\n",
    "    # download_dir=\"/home/amittur/.cache/huggingface/hub\", \n",
    "    max_model_len=10000,\n",
    "    # gpu_memory_utilization=0.95\n",
    "    # max_num_seqs=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLVE_PROMPT = \"\"\"\n",
    "Answer the math problem in the format shown below. End your response with \"<|eot_id|>\".\n",
    "\n",
    "---\n",
    "Problem: <you will be given a math question> \n",
    "Reasoning: <your step by step reasoning for the answer>\n",
    "Answer: <your final answer only>\n",
    "---\n",
    "\n",
    "Problem: {}\n",
    "\"\"\"\n",
    "\n",
    "def get_prompts(data):\n",
    "    return [SOLVE_PROMPT.format(d['question']) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(\n",
    "    temperature=0,\n",
    "    max_tokens=5120,\n",
    "    stop_token_ids=[128001, 128008, 128009],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    match = re.search(r\"Answer: (.+)\", text, re.DOTALL)\n",
    "    answer = ''\n",
    "    reasoning = '[NO_COT_REASONING]'\n",
    "    if match:\n",
    "        # reasoning = match.group(1).strip()\n",
    "        answer = match.group(1).strip()\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'reasoning': reasoning\n",
    "    }\n",
    "\n",
    "def extract_cot_answer(text):\n",
    "    match = re.search(r\"Reasoning: (.+)Answer: (.+)\", text, re.DOTALL)\n",
    "    answer = reasoning = ''\n",
    "    if match:\n",
    "        reasoning = match.group(1).strip()\n",
    "        answer = match.group(2).strip()\n",
    "    return {\n",
    "        'answer': answer,\n",
    "        'reasoning': reasoning\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.70s/it, est. speed input: 26.71 toks/s, output: 30.44 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reasoning: The area of the shaded region is the area of the entire circle minus the area of the unshaded region. The area of the entire circle is $\\pi \\cdot 6^2 = 36\\pi$ square inches. The area of the unshaded region is $\\pi \\cdot 4^2 = 16\\pi$ square inches. Therefore, the area of the shaded region is $36\\pi - 16\\pi = 20\\pi$ square inches. The ratio of the area of the shaded region to the area of the entire circle is $\\frac{20\\pi}{36\\pi} = \\frac{5}{9}$. Since the darts land randomly, we can expect $\\frac{5}{9}$ of the darts to land in the shaded region. Therefore, we can expect $9 \\cdot \\frac{4}{9} = 4$ darts to land in the non-shaded region.\n",
      "Answer: 4\n",
      "{'answer': '4', 'reasoning': 'The area of the shaded region is the area of the entire circle minus the area of the unshaded region. The area of the entire circle is $\\\\pi \\\\cdot 6^2 = 36\\\\pi$ square inches. The area of the unshaded region is $\\\\pi \\\\cdot 4^2 = 16\\\\pi$ square inches. Therefore, the area of the shaded region is $36\\\\pi - 16\\\\pi = 20\\\\pi$ square inches. The ratio of the area of the shaded region to the area of the entire circle is $\\\\frac{20\\\\pi}{36\\\\pi} = \\\\frac{5}{9}$. Since the darts land randomly, we can expect $\\\\frac{5}{9}$ of the darts to land in the shaded region. Therefore, we can expect $9 \\\\cdot \\\\frac{4}{9} = 4$ darts to land in the non-shaded region.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "\n",
    "outputs = llm.generate(SOLVE_PROMPT.format(data[5]['question']), sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "print(extract_cot_answer(outputs[0].outputs[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prompts = get_prompts(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  13%|█▎        | 671/5000 [05:11<34:08,  2.11it/s, est. speed input: 270.40 toks/s, output: 510.89 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:11:52 scheduler.py:1481] Sequence group 926 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  15%|█▌        | 769/5000 [06:05<47:30,  1.48it/s, est. speed input: 262.74 toks/s, output: 533.38 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:12:46 scheduler.py:1481] Sequence group 944 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▋        | 814/5000 [07:19<2:59:53,  2.58s/it, est. speed input: 231.88 toks/s, output: 517.65 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:14:01 scheduler.py:1481] Sequence group 875 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  17%|█▋        | 826/5000 [08:51<20:09:21, 17.38s/it, est. speed input: 195.13 toks/s, output: 459.05 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:17:08 scheduler.py:1481] Sequence group 609 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  17%|█▋        | 874/5000 [18:30<6:18:21,  5.50s/it, est. speed input: 98.99 toks/s, output: 422.23 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:25:18 scheduler.py:1481] Sequence group 880 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  18%|█▊        | 905/5000 [20:00<3:20:49,  2.94s/it, est. speed input: 95.70 toks/s, output: 434.24 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:26:43 scheduler.py:1481] Sequence group 983 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  19%|█▊        | 929/5000 [20:41<2:32:50,  2.25s/it, est. speed input: 94.92 toks/s, output: 439.93 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:27:22 scheduler.py:1481] Sequence group 1042 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  19%|█▉        | 969/5000 [21:25<55:55,  1.20it/s, est. speed input: 95.58 toks/s, output: 447.95 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:28:06 scheduler.py:1481] Sequence group 1110 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|█▉        | 997/5000 [21:56<1:51:39,  1.67s/it, est. speed input: 95.67 toks/s, output: 455.16 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:28:37 scheduler.py:1481] Sequence group 1164 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  21%|██        | 1037/5000 [22:30<43:50,  1.51it/s, est. speed input: 96.84 toks/s, output: 465.31 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:29:15 scheduler.py:1481] Sequence group 1237 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  22%|██▏       | 1105/5000 [23:29<1:28:18,  1.36s/it, est. speed input: 98.37 toks/s, output: 467.71 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:30:11 scheduler.py:1481] Sequence group 1215 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  23%|██▎       | 1134/5000 [25:17<12:32:28, 11.68s/it, est. speed input: 94.02 toks/s, output: 454.73 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:32:09 scheduler.py:1481] Sequence group 1114 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  23%|██▎       | 1174/5000 [31:15<1:15:20,  1.18s/it, est. speed input: 78.92 toks/s, output: 448.33 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:38:00 scheduler.py:1481] Sequence group 1246 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  24%|██▍       | 1196/5000 [31:56<2:00:06,  1.89s/it, est. speed input: 78.96 toks/s, output: 457.90 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:38:40 scheduler.py:1481] Sequence group 1420 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  24%|██▍       | 1217/5000 [32:20<1:03:37,  1.01s/it, est. speed input: 79.36 toks/s, output: 458.88 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:39:00 scheduler.py:1481] Sequence group 1437 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  25%|██▌       | 1255/5000 [32:50<35:48,  1.74it/s, est. speed input: 80.33 toks/s, output: 462.97 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:39:31 scheduler.py:1481] Sequence group 1460 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  28%|██▊       | 1410/5000 [34:09<21:47,  2.75it/s, est. speed input: 84.76 toks/s, output: 471.54 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:40:50 scheduler.py:1481] Sequence group 1551 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  30%|██▉       | 1491/5000 [35:09<32:25,  1.80it/s, est. speed input: 86.62 toks/s, output: 474.81 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:41:50 scheduler.py:1481] Sequence group 1602 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  30%|███       | 1524/5000 [35:40<57:13,  1.01it/s, est. speed input: 86.96 toks/s, output: 474.74 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:42:21 scheduler.py:1481] Sequence group 1600 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  31%|███       | 1549/5000 [38:50<15:29:40, 16.16s/it, est. speed input: 81.11 toks/s, output: 462.53 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:45:40 scheduler.py:1481] Sequence group 1455 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  32%|███▏      | 1588/5000 [41:18<1:12:06,  1.27s/it, est. speed input: 78.09 toks/s, output: 463.00 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:47:58 scheduler.py:1481] Sequence group 1636 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  32%|███▏      | 1601/5000 [43:00<4:22:37,  4.64s/it, est. speed input: 75.71 toks/s, output: 463.76 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:49:45 scheduler.py:1481] Sequence group 1821 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  32%|███▏      | 1611/5000 [43:20<1:32:11,  1.63s/it, est. speed input: 75.53 toks/s, output: 466.52 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:50:05 scheduler.py:1481] Sequence group 1866 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  38%|███▊      | 1924/5000 [45:39<47:54,  1.07it/s, est. speed input: 83.12 toks/s, output: 483.63 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:52:19 scheduler.py:1481] Sequence group 2135 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  41%|████      | 2060/5000 [47:04<48:39,  1.01it/s, est. speed input: 86.75 toks/s, output: 489.77 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:53:45 scheduler.py:1481] Sequence group 2222 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  42%|████▏     | 2078/5000 [47:31<39:44,  1.23it/s, est. speed input: 86.70 toks/s, output: 489.33 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:54:14 scheduler.py:1481] Sequence group 2192 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  43%|████▎     | 2128/5000 [49:14<4:59:33,  6.26s/it, est. speed input: 85.92 toks/s, output: 487.37 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 12:55:57 scheduler.py:1481] Sequence group 2159 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  43%|████▎     | 2164/5000 [55:53<2:58:49,  3.78s/it, est. speed input: 77.05 toks/s, output: 478.54 toks/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:02:38 scheduler.py:1481] Sequence group 2365 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  45%|████▍     | 2233/5000 [56:54<18:05,  2.55it/s, est. speed input: 77.95 toks/s, output: 483.63 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:03:34 scheduler.py:1481] Sequence group 2437 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  47%|████▋     | 2351/5000 [58:24<1:01:57,  1.40s/it, est. speed input: 80.53 toks/s, output: 485.38 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:05:05 scheduler.py:1481] Sequence group 2432 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  47%|████▋     | 2371/5000 [1:00:47<13:27:19, 18.43s/it, est. speed input: 78.32 toks/s, output: 476.15 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:07:43 scheduler.py:1481] Sequence group 2295 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  62%|██████▏   | 3095/5000 [1:10:06<16:58,  1.87it/s, est. speed input: 88.21 toks/s, output: 486.28 toks/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:16:46 scheduler.py:1481] Sequence group 3325 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|██████▉   | 3493/5000 [1:13:20<39:14,  1.56s/it, est. speed input: 98.28 toks/s, output: 488.25 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:20:02 scheduler.py:1481] Sequence group 3548 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  70%|███████   | 3521/5000 [1:17:29<2:20:29,  5.70s/it, est. speed input: 93.91 toks/s, output: 483.26 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:24:12 scheduler.py:1481] Sequence group 3600 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  74%|███████▍  | 3698/5000 [1:19:34<14:31,  1.49it/s, est. speed input: 97.76 toks/s, output: 485.64 toks/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:26:15 scheduler.py:1481] Sequence group 3793 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|███████▌  | 3796/5000 [1:20:53<38:57,  1.94s/it, est. speed input: 99.50 toks/s, output: 485.24 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:27:34 scheduler.py:1481] Sequence group 3885 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  77%|███████▋  | 3861/5000 [1:21:44<07:56,  2.39it/s, est. speed input: 99.83 toks/s, output: 489.03 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:28:29 scheduler.py:1481] Sequence group 4086 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  94%|█████████▎| 4680/5000 [1:26:24<02:03,  2.59it/s, est. speed input: 111.18 toks/s, output: 503.98 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-26 13:33:04 scheduler.py:1481] Sequence group 4900 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5000/5000 [1:36:20<00:00,  1.16s/it, est. speed input: 105.98 toks/s, output: 511.36 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = llm.generate(data_prompts, sampling_params)\n",
    "answers = []\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    # prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "\n",
    "    answers.append(extract_cot_answer(generated_text))\n",
    "\n",
    "    # print(f\"Question Idx: {i}, Response: {generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save answers as json\n",
    "with open(\"answers_70b_int4.json\", \"w\") as f:\n",
    "    json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "manyshot-math",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
